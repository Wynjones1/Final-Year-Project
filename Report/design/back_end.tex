\section{Back End}
The back-end contains the code that deals with processing scene data, this block is decomposed into
two main blocks, the photon mapping block and the raytracing block, the photon mapping block is responsible
for the preprocessing step that creates the photon maps used to estimate the radiance. The raytracing stage
is responsible for computing the colour of each of the pixels in the output image, this will use the data
that was read into from the front end and the photon maps that were created by the photon map module. The
output of the raytracing block is sent to the front end which will use the data to present or save the final
image. The design of these blocks is that of a pipeline where certain procedures are performed on the scene
data and the results of the process is passed to the next stage, both pipelines contain a stage whereby
data can be passed to multiple threads to be process, each of these threads communicate to the next
stage in the pipeline through a queue that acts as the inter-thread communication method.

\subsection{Photon Emmition and Photon Map Construction (1 page)}
This block contains two stages, the photon emmition stage which traces photons through the scene and stores
the point of absorbtion and the balancing stage which sorts the photons into a data structure (left balanced
tree) that facilitated efficient storage and searching. The reason for the balancing stage is that the
distriburtion of the photons in the map is unlikely to be optimal \todo{Cite jensen book} and as this is a
preprocessing step it is worth the time to perform the balancing to improve later stages.

The construction of the photon map can be computationally expensive when a high number of photons are
requested to be created, in order to improve the performance of this stage multiple threads are created
that are responsible for emmitting a certain number of photons in the map, the number of threads will be
set in the front end and is configurable by the user to suit the machine that the system is being run on.

Each of the threads will trace photons through the scene as described in Algorithm\todo{Emmition Algorithm Ref}
once a photon has been found that will be stored in the photon map it will be written to a queue that is shared
by each of the threads, the photons have their power scaled as they are read from the queue.

The next stage in the pipeline reads photons from the queue, this will continue until each of the threads
has signalled that they have completed processing the photons, once all of the threads have been completed the
the list of photons will be transftered into the photon maps, this is the balancing stage.

\begin{figure}
\centering
\includegraphics{./images/photon_threading.png}
\caption{Photon map generation}
\label{fig:photon_generation}
\end{figure}

\subsubsection{Multiple Photon Maps}
During the construction of the photon maps a choice must be made as to which of the photon maps the photon
should be placed in, this requires additional information to be passed from the emmition thread, this information
will describe the path that the photon has taken, this will in essence be an implementation of the path notation
that is commonly used to describe the path of a photon, in this case all paths will be stored in the global photon
map (\textbf{L(S\textbar D)$^+$D} in the path notation),only specular paths are stored in the caustic photon map
as these are the paths that contribute the most to the focusing effect of caustics (\textbf{LS$^+$D})


\subsection{Raytracing (1 pages)}
As can be seen from Figure~\ref{fig:design_blocks} the raytracing module contains $n$ threads, each of these
threads trace a subset of the pixels in the image and run independantly to each other, this is possible due
to the inherent parrellism in the photon mapping algorithm. This stage in the system will perform raytracing
as described in \todo{cite}, for each intersection found during the running of the system the photon
maps created during previous stage will be used to estimate the radiance of the point in order to estimte the
true illumination at the point factoring in the global effects.

\begin{figure}
\centering
\includegraphics{./images/pixel_threading.png}
\caption{Raytracing architecture}
\label{fig:pixel_generation}
\end{figure}

The architecture of the raytracing block shared many simalarities with that of the photon emittion stage,
it is also a pipeline, there is a threads that inputs each pixel in the output image to a queue, this queue
is in turn read by one of multiple threads that perform photon mapping for a ray emmited through the pixel,
multiple rays are generated for each pixel performing distrubution raytracing, the colour of the pixel is
then written to an output queue that will be read by the main thread in order to send the pixel values to
the front end to be presented to the user.

\subsubsection{Distribution Raytracing}
Each of the raytracing threads performs distribution raytracing, this is a commonly used adjustment to the 
raytracing algorithm that allows for simulation of fuzzy phenomena such as soft shadows and glossy reflection
models, as described by Jensen \todo{cite} distribution raytracing can also be used to estimate the contribuiton
of diffuse intereflection, at each intersection of the eye ray a diffuse refeltive \todo{this is only true of
diffuse surfaces} ray is generated, this ray is then traced into the scene, at the intersection point of this ray
we use the photon map to estimate the radiance leaving the surface in the direction of the original intersection
point, as we are tracing multiple rays per pixel an estimate of the global illumination is found by averaging each
of these reflective rays.

\begin{figure}
\missingfigure{Algorithm For distributiuon raytracing}
\end{figure}

\subsection{Objects}
Much of the raytracing algorithm that is performed on each of the
objects in a scene are the same, for example finding a refracted
ray is only dependant on the surface noraml at a point of intersection
for this reason the system abstracts any object that can be intersected
must implement certain functions, this is a form of object orentation.

\subsection{Materials}
Each object has associated a material property that determines how light interacts with the surface of the
object, in this system we consider diffuse and perfectly specular materials componenets that will contribute
to the reflectance of the object, each term is split into three turns for red, green and blue components.
\todo{improve the phrasing of this}

\subsection{Participating Media}
In a simple raytracer as a ray is traced through a scene the radiance is assumed to be constant along the path,
in the real world this is not true due to lights interaction with particals in the air, these interactions
are commonly refered to as scattering, two types of scattering are possible, in and out scattering, the latter
causes a decrease in the radiance along a path and the former an increase in the scattering, this effect is
responsible for blue skies as light is scattered different amounts dependant on the frequency. In this project
the system is able to simulate this effect by applying a participating media to an object, We will only consider
the effect of scattering in three colour bands. two components are used to describe the
properties of a participating media, the scattering and absorbtion coefficient. \todo{Add some citations}

\subsection{Shading}
Each thread is responsible for performing that shading at the intersection of eye rays, for specular reflection
and transmission this results in an additional raytrace be performed to determin the shading colour, for diffuse
surfaces a direct illumination calculationn is performed, additionally the photon map is queried to determin
the indirect lighting. For participating media, the radiance is calculated by performing by a ray-marching
procedure.
